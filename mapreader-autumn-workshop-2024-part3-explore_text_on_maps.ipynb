{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MapReader Autumn Workshop (2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up for Google Colab\n",
    "\n",
    "The below cells will:\n",
    "\n",
    "- Mount your Google Drive\n",
    "- Create a directory for the workshop\n",
    "- Change the working directory to the workshop directory\n",
    "- Download and install the required packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mount your drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# set up MapReader_Autumn_Workshop directory\n",
    "!mkdir /content/drive/MyDrive/MapReader_Autumn_Workshop\n",
    "%cd /content/drive/MyDrive/MapReader_Autumn_Workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/maps-as-data/mapreader-autumn-workshop-2024.git\n",
    "!pip install mapreader[dev]\n",
    "!pip install sentence-transformers scikit-learn plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# enable custom widgets in colab\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Text on Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 'Urban' vs 'Rural' text\n",
    "\n",
    "This notebook provides some examples of ways in which MapReader's patch classification and text spotting outputs can be combined.\n",
    "\n",
    "We will use our datasets to investigate the textual description of urban and rural landscapes by comparing text that often appears in the built environment (i.e. near building patches) versus the rest of the map. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from collections import Counter\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "We will first need to load our building classification data and text spotting data.\n",
    "\n",
    "Since these were saved as `geojson` files we can load them with the `geopandas` library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **NOTE**: If you annotated/trained a model for something other than buildings, you will need to save the building predictions from [here](https://drive.google.com/file/d/1xrD5TQz_ILASsx17qScGbcH5OLq1XeE8/view?usp=drive_link) to the workshop directory in you Google Drive. You should then update the path in the cell below to \"./building_predicted_outputs.geojson\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load patch predictions and spotted text\n",
    "predictions = gpd.read_file(\"./predicted_outputs.geojson\")\n",
    "spotted_text = gpd.read_file(\"./deepsolo_text_predictions.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spotted_text.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we will convert all our patch and text polygons to centroids (i.e. the point in the middle of the polygon).\n",
    "\n",
    "This will make distance calculations easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert polygons to centroids\n",
    "predictions['centroid'] = predictions['geometry'].centroid \n",
    "spotted_text['centroid'] = spotted_text['geometry'].centroid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell identifies the text that falls within the 100m of building patches. \n",
    "\n",
    "Text within this distance is stored as \"adjacent text\" and any other text is stored as \"other text\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacent_text = [] # here we store text close to the target category\n",
    "other_text = [] # here we store the other text\n",
    "\n",
    "target_label = \"building\"\n",
    "\n",
    "for i, row in tqdm(predictions.iterrows()):\n",
    "    # get text within a certain distance from the patch centroid\n",
    "    text = spotted_text[spotted_text.to_crs(epsg=27700).distance(row.centroid) <= 100][\"text\"].tolist()\n",
    "    # if patch is classified as the target label, add text to adjacent_text, otherwise add to other_text\n",
    "    if row['predicted_label'] == target_label:\n",
    "        adjacent_text.extend(text)\n",
    "    else:\n",
    "        other_text.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Text near to buildings: {len(adjacent_text)}\") \n",
    "print(f\"Text far from buildings: {len(other_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find probabilities of each word/phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and probabilities of the text labels for the building category\n",
    "building_text_freq =  Counter([i.lower() for i in adjacent_text])\n",
    "building_text_prob = {k : v / sum(building_text_freq.values()) for k, v in building_text_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get counts and probabilities of the text labels for the other category\n",
    "other_text_freq =  Counter([i.lower() for i in other_text])\n",
    "other_text_prob = {k: v / sum(other_text_freq.values()) for k, v in other_text_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare both absolute counts and probabilities of a give word\n",
    "word = 'street'\n",
    "print(building_text_freq[word], other_text_freq[word])\n",
    "print(building_text_prob[word], other_text_prob[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the proportional difference\n",
    "proportional_difference = sorted({w: building_text_prob.get(w,0) - other_text_prob.get(w,0) for w in other_text_prob.keys()}.items(), key=lambda x: x[1], reverse=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Building labels: {proportional_difference[:5]}')\n",
    "print(f'Other labels: {proportional_difference[-5:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(proportional_difference[:10]).plot(kind='bar', x=0, y=1, legend=False, \n",
    "                            title='Top 10 terms in Building labels', \n",
    "                            xlabel='Term', ylabel='Difference in probability')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(proportional_difference[-10:]).plot(kind='bar', x=0, y=1, legend=False, \n",
    "                            title='Top 10 terms in Other labels', \n",
    "                            xlabel='Term', ylabel='Difference in probability')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of what some of the abbreviations mean, please go to the NLS website: https://maps.nls.uk/os/abbrev/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visalizing the semantic of text on maps\n",
    "\n",
    "In the visualization below we encode each label to a vector using BERT-type language model. This generates a vector for each labels that approximates the 'meaning' of this label. Then we visualize these embeddigns in two dimensional space where you can explore the different semantic regions of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all text labels\n",
    "text_labels = spotted_text.text.str.lower().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained sentence transformer model\n",
    "# if you are working with a different language, you can change the model to a multilingual one\n",
    "# please refer to the documentation for more information: https://www.sbert.net/docs/pretrained_models.html\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "\n",
    "# encode the sentences\n",
    "sentence_embeddings = model.encode(text_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform dimensionality reduction using TSNE\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(sentence_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize the labels in 2D scatter plot\n",
    "data = pd.DataFrame(embeddings_tsne, columns=['x','y'])\n",
    "data['text'] = text_labels\n",
    "fig = px.scatter(data, x=\"x\", y=\"y\", text='text', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize only the text labels in 2D scatter plot\n",
    "# i.e. remove all numbers\n",
    "data_text = data[data.text.str.isalpha()]\n",
    "fig = px.scatter(data_text, x=\"x\", y=\"y\", text='text', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize only the unique text labels in 2D scatter plot\n",
    "# i.e. remove all numbers and duplicates\n",
    "data_text_unique =data[data.text.str.isalpha()].drop_duplicates(subset='text')\n",
    "fig = px.scatter(data_text_unique, x=\"x\", y=\"y\", text='text', width=1000, height=1000,)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ce",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
